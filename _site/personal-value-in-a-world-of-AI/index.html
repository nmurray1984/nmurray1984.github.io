<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Personal Value in an AI World | Nathan Murray</title>
  <meta name="description" content="Personal website">
  <meta name="author" content="Nathan Murray">

  <!-- Open Graph -->
  <meta property="og:title" content="Personal Value in an AI World">
  <meta property="og:description" content="Personal website">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://localhost:4000/personal-value-in-a-world-of-AI/">

  <!-- Favicon -->
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>üë§</text></svg>">

  <!-- Styles -->
  <link rel="stylesheet" href="/assets/css/style.css">
</head>
<body>
  <div class="container">
    <header>
      <nav>
        <a href="/" class="site-title">Nathan Murray</a>
      </nav>
    </header>

    <main>
      <article>
  <header class="post-header">
    <h1>Personal Value in an AI World</h1>
    <time datetime="2023-11-28T00:00:00-06:00">November 28, 2023</time>
  </header>

  <div class="post-content">
    <p>This essay seeks to answer the question, ‚ÄúWhat is personal value in a world of AI?‚Äù. As I‚Äôm writing this in 2023, AI is all the buzz. OpenAI‚Äôs release of ChatGPT last year has completely shifted the strategy of many companies and became the catalyst for many startups. No one knew that the underlying technology (the transformer neural network) would be so successful at simulating human writing. A shift of this magnitude has brought significant polarization in around the implications - some say that it will be the best thing mankind has ever done, and some say it will destroy us all. Most agree that many jobs will either change substantially or be completely replaced. With such substantial change right around the corner, how can we prepare, as individuals, as employees, and as managers?</p>

<p>First, a caveat. Some believe that the possibility for self-replicating and self-aware AI is right around the corner. If that happens, it will bring with it its own set of implications, many for which no one can prepare for in any meaningful way. For this essay, let‚Äôs assume that progress in AI centers around improvements in speed, reliability, tool usage, and reasoning.</p>

<p>The hardest part of preparing for an AI-driven future is figuring out just what we think will actually happen. According to the popular media, we‚Äôll either all die or all be rich. The truth must be somewhere in between. The issue is complicated by the fact that the field is improving so rapidly. For this reason, asking what capabilities AI will have in the future is a moving target and is up to dramatic levels of interpretation. Instead, let‚Äôs ask the question: <strong>What are the capabilities AI will <em>never</em> have?</strong> Going about it this way, we have a solid target and can better explore what the end state we believe is reasonable.</p>

<h2 id="what-is-personal-value">What Is Personal Value?</h2>

<p>The first thing that we should discuss is the concept of value. Value is a multi-faced topic that means different things to different people. Perhaps lets start with an example. Today you are a software engineer with 10 years of experience in Java. Your boss tells you one day that your role will be replaced by an AI agent that has been trained on the company‚Äôs codebase and will write code faster and better than you ever could. Your boss intends to chat with it and asks it to build the features you were once tasked with. You are clearly being replaced by a machine.</p>

<p>Could this actually happen? To many it is an all too clear inevitable future. But it brings up the more underlying question - what are the differences between man and machine? Or more specifically, to what extent can a man be replaced by a machine?</p>

<p>To answer these questions I think it best not to start by defining how close a machine can get to being human, but rather what are the things we know a machine will never do. In this thought experiment, we should assume that the current restrictions on AI - memory,  speed, and cost - have been overcome to the extent that we don‚Äôt need to consider them. Given the pace of change in AI, I think it reasonable to assume these barriers will be overcome in the coming years.</p>

<h2 id="human-experience">Human Experience</h2>

<p>First, a machine does not have the experience of a human. It‚Äôs never been born, or raised. It doesn‚Äôt have parents, it doesn‚Äôt have a body. It doesn‚Äôt need food and has never felt hungry or full. It has never felt thirsty or tired. It‚Äôs never fallen on the playground and scraped its knees. It‚Äôs never looked around and couldn‚Äôt find mommy. It‚Äôs never been to school and made friends. It‚Äôs never felt the joy of real friendship nor the pain when a friendship sours. It‚Äôs never gotten into a fight and learned to standup to bullies. It‚Äôs never gotten a bad grade or botched a report at work. It‚Äôs never raised children nor had to take care of aging parents.</p>

<p>What‚Äôs interesting to me is that modern AIs like GPT-4 have learned all about what humans have said about these things, and is really great at simulating what humans would do and how they would feel in all these scenarios. In a way it ‚Äúknows‚Äù and ‚Äúunderstands‚Äù these experiences without having the experience itself. And it‚Äôs not too much of a mental leap to extrapolate out a few years that these systems will be even better at understanding human experience and emotion.</p>

<h2 id="mediated-senses">Mediated Senses</h2>

<p>Second, machines are reliant on humans for their input and output. Humans understand the real world through our senses. And we learn to trust those senses over time. As a baby, you learn that the toy you see is also something you can reach out and touch. The smell of good food indicates the presence of something that you can see with your eyes and touch with your hands. The senses reinforce themselves and work together to develop an understanding of that which is real. Now the philosophers will begin to ask ‚Äúbut what is real‚Äù? ‚Ä¶ and my reply is that there is a common understanding of what is real across humanity. My point is not to discuss whether what we experience is real. Rather, that what we experience is real to us and there exists a systematic reality outside of each human that we interact with.</p>

<p>What‚Äôs more important here is that for humans our bodies are what create our senses for us. We cannot design our eyes or ears (although we may improve them through glasses or hearing aids). When a machine is interacting with its environment, humans create the systems for which it experiences the world. Humans mediate the ‚Äúexperience‚Äù of the AI. An AI does not know that it‚Äôs output is going to a human, or to an agent, unless it‚Äôs told that‚Äôs the case.</p>

<p>What this means is that humans will always have a role in shaping AI. And that humans have a unique role in shaping how AIs perceive reality. There‚Äôs no way that an AI can ‚Äútrust‚Äù it‚Äôs own senses. All input is decided by humans.</p>

<p>Now this does get turned on its head if we look into a future where autonomous robots are in our houses helping us with household chores and playing ball with the kids outside. But even this type of experience is mediated by humans - we can turn machines off.</p>

<p>The implication here is that time is no longer a constant progression for a machine. As humans, our consciousness is a stream of sensed information and thoughts. There are times when we have nothing new going on, and we call this boredom. Machines are exempt from this constant continuance of time.</p>

<h2 id="pain">Pain</h2>

<p>Third, a machine can‚Äôt be punished for doing something wrong in any real sense. Punishment is a key deterrent of bad actors; for instance, if you kill someone and are convicted in court you will likely go to prison for the rest of your life. If you accidentally kill someone and are convicted (manslaughter) you may go to prison, or at the least probation for some time.</p>

<p>Machines are not the same way. If a machine kills someone, say it determines in HAL-9000 style that it must kill someone to achieve its given mission, there would be three options. First would be to turn it off and delete its files. Second would be to give it a tedious task a la Sisyphus - not really helping to provide justice. The third would be to make a decision between holding those accountable that created the machine, ruling that the person owning or in charge of the machine was liable, using the same logic you would if someone accidentally killed themselves playing with a gun.</p>

<p>The third option is obviously the only real choice. You can‚Äôt send a machine to prison, give it probation, or punish it in any real way. The implication here is that there will always need to be a human in the loop when machines are given dangerous responsibilities. This is already true. Aircraft have been able to fly themselves for decades, but we still have a pilot onboard. Why? Because pilots can be reprimanded, fired, and sent to prison. This negative incentive is necessary to keep trust in a system where people are in an inherently dangerous situation moving at six hundred miles per hour to get to their destination. No matter how safe planes are - there‚Äôs always a non-zero chance of catastrophic failure. And that means that a human must be held accountable if something goes wrong. If auto-pilot reached a point where pilots were not necessary on board, and people died in a crash, one can only imagine the questions regulators would ask of the company that build and tested the software - holding them liable for not only the software, but the situations in which the software was put in control.</p>

<h2 id="where-humans-will-continue-to-thrive">Where humans will continue to thrive</h2>

<p>The 3 things humans have that machine can‚Äôt have - experience, trusted senses, and pain, help us to frame what a world will look like with incredibly intelligent agents in the wild.</p>

<ol>
  <li>We can be sure that humans will still be needed in empathetic roles (therapy, medicine, friendships, parenting, mentoring, financial advisory, teaching)</li>
  <li>We can be sure that humans will still be building things - that is taking a vision and making it real - although this will be more directing (tell, sense, correct loop) than doing (build, sense, correct). This also means that humans will be responsible for the narration of the truth, held responsible to answer the question ‚Äúwhat happened?‚Äù and held accountable when lying.</li>
  <li>We can be sure that humans will still be accountable for risky activities - both risky for an individual (an executive that trusts a friend to deliver something when jobs are on the line), or dangerous (pilots, big construction projects, etc.).</li>
</ol>

<p>And all of this doesn‚Äôt discount the fact that these systems have many things that they are very good at. Language Translation is just about a solved problem now. LLMs can sense and respond to meaning and nuance in language. LLMs rarely make spelling or grammar mistakes. In this way I like to think that we‚Äôve already created C3PO, very smart but also can be very dumb and comical.</p>

<h2 id="assumptions-for-the-future">Assumptions for the future</h2>

<p>So where does that put humans, especially when it comes to work? It all comes down to value. What is the value that we provide outside of what an AI could do? Well first we need to make some assumptions about the future to constrain our thinking. First, we‚Äôll assume that AI cannot just gain consciousness. We‚Äôll cover that later, but for the first part, it will simplify our analysis. Second, we‚Äôll assume that the capabilities of LLMs and diffusion models today will continue to progress but we won‚Äôt see significant new capabilities and failure modes will be present but diminished. For instance, GPT-5 and GPT-6 (or company‚Äôs versions) will cover more topics, be right more often than not, be much more stearable and explainable, be more trainable with custom data, and will run faster and cheaper. Among all these dimensions there could be step level improvements in the next few years. But there will still be problems with hallucinations and incorrect answers in some cases. Thirdly, we‚Äôll assume that tooling and integrations on top of these LLMs will expand dramatically. AI will be a new user interface (perhaps the primary?) to computers / phones and a first-class feature for all apps. Context window limitations will all but disappear, giving way to full code-base and organization-wide document analysis. I expect all of these things in the next 1-5 years.</p>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>So when all of these things are in place, what is the ‚Äújob‚Äù of a human? How can a human compete with intelligence at this scale?</p>

<ol>
  <li>Sensing / Storytelling Value - Providing a feedback loop to an AI; quality control; input of what‚Äôs all going on; prompting; ‚Äúknowing‚Äù the AI; trusting senses and storytelling of all that happened</li>
  <li>Outcomes Value - Being trusted to deliver an outcome under reasonable circumstances and accountable for the job.</li>
  <li>Relationship Value - Being empathetic; connecting on a human level; parenting; having fun together. Having children together. Love.</li>
  <li>Leadership Value - Holding a strong perspective, motivating others, persistence, grit, etc.</li>
</ol>

  </div>
</article>

    </main>

    <footer>
      <p>&copy; 2026 Nathan Murray</p>
    </footer>
  </div>
</body>
</html>
